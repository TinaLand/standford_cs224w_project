# RL Agent è¯¦è§£ï¼šåŠŸèƒ½ã€åŸç†ä¸å·¥ä½œæµç¨‹

## ğŸ“‹ ç›®å½•
1. [Agent å¯ä»¥åšä»€ä¹ˆ](#agent-å¯ä»¥åšä»€ä¹ˆ)
2. [Agent çš„å·¥ä½œåŸç†](#agent-çš„å·¥ä½œåŸç†)
3. [Agent ä¸ RL çš„é›†æˆ](#agent-ä¸-rl-çš„é›†æˆ)
4. [å®Œæ•´å·¥ä½œæµç¨‹](#å®Œæ•´å·¥ä½œæµç¨‹)

---

## 1. Agent å¯ä»¥åšä»€ä¹ˆ

### ğŸ¯ æ ¸å¿ƒåŠŸèƒ½

`StockTradingAgent` æ˜¯ä¸€ä¸ª**æ™ºèƒ½äº¤æ˜“ä»£ç†**ï¼Œå®ƒå¯ä»¥ï¼š

#### 1.1 **å­¦ä¹ äº¤æ˜“ç­–ç•¥**
- é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è‡ªåŠ¨å­¦ä¹ ä½•æ—¶ä¹°å…¥ã€å–å‡ºæˆ–æŒæœ‰è‚¡ç¥¨
- ä¸éœ€è¦äººå·¥ç¼–å†™äº¤æ˜“è§„åˆ™ï¼Œè€Œæ˜¯ä»å†å²æ•°æ®ä¸­å­¦ä¹ æœ€ä¼˜ç­–ç•¥

#### 1.2 **åšå‡ºäº¤æ˜“å†³ç­–**
- æ ¹æ®å½“å‰å¸‚åœºçŠ¶æ€ï¼ˆè‚¡ç¥¨ä»·æ ¼ã€GNN åµŒå…¥ç‰¹å¾ã€æŒä»“æƒ…å†µï¼‰å†³å®šäº¤æ˜“åŠ¨ä½œ
- ä¸ºæ¯åªè‚¡ç¥¨é€‰æ‹©ï¼š**ä¹°å…¥ (2)**ã€**æŒæœ‰ (1)** æˆ– **å–å‡º (0)**

#### 1.3 **ä¼˜åŒ–æŠ•èµ„ç»„åˆ**
- åŠ¨æ€è°ƒæ•´æŒä»“æ¯”ä¾‹ï¼Œæœ€å¤§åŒ–æ”¶ç›ŠåŒæ—¶æ§åˆ¶é£é™©
- è€ƒè™‘äº¤æ˜“æˆæœ¬ã€æ»‘ç‚¹ã€é£é™©è°ƒæ•´æ”¶ç›Šï¼ˆSharpe Ratioï¼‰

#### 1.4 **è¯„ä¼°äº¤æ˜“è¡¨ç°**
- è®¡ç®—ç´¯è®¡æ”¶ç›Šã€Sharpe Ratioã€æœ€å¤§å›æ’¤ç­‰æŒ‡æ ‡
- æ”¯æŒå¤šè½®è¯„ä¼°ï¼Œç»Ÿè®¡å¹³å‡è¡¨ç°å’Œç¨³å®šæ€§

---

## 2. Agent çš„å·¥ä½œåŸç†

### ğŸ—ï¸ æ¶æ„ç»„æˆ

```
StockTradingAgent
â”œâ”€â”€ GNN Model (RoleAwareGraphTransformer)
â”‚   â””â”€â”€ æå–è‚¡ç¥¨å›¾åµŒå…¥ç‰¹å¾
â”œâ”€â”€ PPO Agent (Stable Baselines3)
â”‚   â””â”€â”€ å­¦ä¹ äº¤æ˜“ç­–ç•¥
â””â”€â”€ Environment (StockTradingEnv)
    â””â”€â”€ æ¨¡æ‹Ÿäº¤æ˜“ç¯å¢ƒ
```

### 2.1 **GNN æ¨¡å‹çš„ä½œç”¨**

```python
# GNN æ¨¡å‹å°†è‚¡ç¥¨å›¾è½¬æ¢ä¸ºç‰¹å¾å‘é‡
graph_t â†’ GNN â†’ embeddings [N Ã— H]
# N = è‚¡ç¥¨æ•°é‡, H = åµŒå…¥ç»´åº¦ (256)
```

**åŠŸèƒ½**ï¼š
- è¾“å…¥ï¼šå¼‚æ„å›¾ `G_t`ï¼ˆåŒ…å«è‚¡ç¥¨èŠ‚ç‚¹ã€å¤šç§è¾¹å…³ç³»ï¼‰
- è¾“å‡ºï¼šæ¯ä¸ªè‚¡ç¥¨çš„åµŒå…¥å‘é‡ï¼ˆæ•è·è‚¡ç¥¨ä¹‹é—´çš„å…³ç³»å’Œç‰¹å¾ï¼‰
- **ä¸ºä»€ä¹ˆéœ€è¦**ï¼šä¼ ç»Ÿ RL åªçœ‹ä»·æ ¼ï¼ŒGNN èƒ½ç†è§£è‚¡ç¥¨ä¹‹é—´çš„å¤æ‚å…³ç³»ï¼ˆè¡Œä¸šã€ä¾›åº”é“¾ã€ç›¸å…³æ€§ç­‰ï¼‰

### 2.2 **PPO Agent çš„ä½œç”¨**

```python
# PPO æ˜¯ä¸€ä¸ªç­–ç•¥ä¼˜åŒ–ç®—æ³•
observation â†’ Policy Network â†’ action probability â†’ action
```

**åŠŸèƒ½**ï¼š
- è¾“å…¥ï¼šçŠ¶æ€è§‚å¯Ÿï¼ˆæŒä»“ + GNN åµŒå…¥ï¼‰
- è¾“å‡ºï¼šäº¤æ˜“åŠ¨ä½œï¼ˆæ¯åªè‚¡ç¥¨çš„æ“ä½œï¼‰
- **å­¦ä¹ æ–¹å¼**ï¼šé€šè¿‡è¯•é”™ï¼Œæœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ï¼ˆæ”¶ç›Šï¼‰

**PPO çš„ä¼˜åŠ¿**ï¼š
- ç¨³å®šè®­ç»ƒï¼ˆé¿å…ç­–ç•¥æ›´æ–°è¿‡å¤§ï¼‰
- é€‚åˆè¿ç»­å†³ç­–é—®é¢˜
- åœ¨é‡‘èé¢†åŸŸè¡¨ç°è‰¯å¥½

### 2.3 **Agent ç±»çš„å°è£…**

```python
class StockTradingAgent:
    def __init__(self, gnn_model, env_factory, ...):
        self.gnn_model = gnn_model  # å†»ç»“çš„ GNNï¼Œç”¨äºç‰¹å¾æå–
        self.agent = PPO(...)       # PPO ç­–ç•¥ç½‘ç»œ
    
    def train(self, timesteps):
        # è®© PPO agent åœ¨ç¯å¢ƒä¸­å­¦ä¹ 
        self.agent.learn(total_timesteps=timesteps)
    
    def predict(self, observation):
        # ä½¿ç”¨è®­ç»ƒå¥½çš„ç­–ç•¥åšå†³ç­–
        return self.agent.predict(observation)
```

**å°è£…çš„å¥½å¤„**ï¼š
- ç»Ÿä¸€æ¥å£ï¼š`train()`, `predict()`, `evaluate()`, `save()`, `load()`
- éšè—å¤æ‚æ€§ï¼šç”¨æˆ·ä¸éœ€è¦ç›´æ¥æ“ä½œ PPO æˆ–ç¯å¢ƒ
- æ˜“äºæ‰©å±•ï¼šå¯ä»¥è½»æ¾æ›¿æ¢ä¸åŒçš„ RL ç®—æ³•

---

## 3. Agent ä¸ RL çš„é›†æˆ

### ğŸ”„ å®Œæ•´äº¤äº’æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Environment â”‚  (StockTradingEnv)
â”‚              â”‚
â”‚  - å½“å‰æ—¥æœŸ   â”‚
â”‚  - è‚¡ç¥¨ä»·æ ¼   â”‚
â”‚  - æŒä»“æƒ…å†µ   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 1. è·å–å½“å‰çŠ¶æ€
       â”‚    (observation)
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GNN Model  â”‚  (RoleAwareGraphTransformer)
â”‚             â”‚
â”‚  - åŠ è½½å›¾ G_tâ”‚
â”‚  - æå–åµŒå…¥  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 2. ç”ŸæˆåµŒå…¥ç‰¹å¾
       â”‚    [portfolio] + [embeddings]
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PPO Agent  â”‚  (åœ¨ StockTradingAgent ä¸­)
â”‚             â”‚
â”‚  - ç­–ç•¥ç½‘ç»œ  â”‚
â”‚  - ä»·å€¼ç½‘ç»œ  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 3. é¢„æµ‹åŠ¨ä½œ
       â”‚    action = [0,1,2,0,2,...]
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Environment â”‚
â”‚              â”‚
â”‚  - æ‰§è¡Œäº¤æ˜“   â”‚
â”‚  - è®¡ç®—æ”¶ç›Š   â”‚
â”‚  - æ›´æ–°çŠ¶æ€   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 4. è¿”å›å¥–åŠ±å’Œä¸‹ä¸€çŠ¶æ€
       â”‚    (reward, next_obs)
       â–¼
    (å¾ªç¯ç»§ç»­...)
```

### 3.1 **çŠ¶æ€è¡¨ç¤º (Observation)**

çŠ¶æ€ç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼š

```python
observation = [
    # 1. å½“å‰æŒä»“ (N ç»´)
    portfolio_holdings = [0.1, 0.2, 0.0, 0.3, ...]  # æ¯åªè‚¡ç¥¨çš„æŒä»“æ¯”ä¾‹
    
    # 2. GNN åµŒå…¥ç‰¹å¾ (N Ã— H ç»´)
    gnn_embeddings = [
        [0.5, -0.2, 0.8, ...],  # è‚¡ç¥¨ 1 çš„åµŒå…¥ (256 ç»´)
        [0.3, 0.1, -0.5, ...],  # è‚¡ç¥¨ 2 çš„åµŒå…¥
        ...
    ]
]

# æ€»ç»´åº¦ = N + (N Ã— H) = N Ã— (1 + H)
```

**ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡**ï¼š
- **æŒä»“ä¿¡æ¯**ï¼šAgent éœ€è¦çŸ¥é“å½“å‰æŠ•èµ„ç»„åˆçŠ¶æ€
- **GNN åµŒå…¥**ï¼šæä¾›è‚¡ç¥¨çš„å¸‚åœºç‰¹å¾å’Œå…³ç³»ä¿¡æ¯
- **ç»„åˆèµ·æ¥**ï¼šAgent å¯ä»¥åŒæ—¶è€ƒè™‘"æˆ‘æŒæœ‰ä»€ä¹ˆ"å’Œ"å¸‚åœºæ˜¯ä»€ä¹ˆçŠ¶æ€"

### 3.2 **åŠ¨ä½œç©ºé—´ (Action Space)**

```python
action_space = MultiDiscrete([3] * N)
# æ¯åªè‚¡ç¥¨æœ‰ 3 ä¸ªé€‰æ‹©ï¼š
# 0 = å–å‡º (Sell)
# 1 = æŒæœ‰ (Hold)  
# 2 = ä¹°å…¥ (Buy)

# ä¾‹å¦‚ï¼šaction = [2, 1, 0, 2, 1, ...]
# è¡¨ç¤ºï¼šä¹°å…¥è‚¡ç¥¨1ï¼ŒæŒæœ‰è‚¡ç¥¨2ï¼Œå–å‡ºè‚¡ç¥¨3ï¼Œä¹°å…¥è‚¡ç¥¨4ï¼Œ...
```

### 3.3 **å¥–åŠ±å‡½æ•° (Reward)**

```python
# åœ¨ StockTradingEnv.step() ä¸­è®¡ç®—
reward = (
    portfolio_return          # ç»„åˆæ”¶ç›Š
    + sharpe_ratio * 0.1      # é£é™©è°ƒæ•´æ”¶ç›Š
    - trading_cost            # äº¤æ˜“æˆæœ¬æƒ©ç½š
    - excessive_trading * 0.01 # è¿‡åº¦äº¤æ˜“æƒ©ç½š
)
```

**è®¾è®¡ç†å¿µ**ï¼š
- ä¸ä»…çœ‹æ”¶ç›Šï¼Œè¿˜è€ƒè™‘é£é™©ï¼ˆSharpe Ratioï¼‰
- æƒ©ç½šé¢‘ç¹äº¤æ˜“ï¼ˆé™ä½äº¤æ˜“æˆæœ¬ï¼‰
- é¼“åŠ±ç¨³å®šã€å¯æŒç»­çš„ç­–ç•¥

### 3.4 **è®­ç»ƒè¿‡ç¨‹ (Training)**

```python
# åœ¨ agent.train() ä¸­
for timestep in range(total_timesteps):
    # 1. Agent ä¸ç¯å¢ƒäº¤äº’
    action = agent.predict(observation)
    next_obs, reward, done, info = env.step(action)
    
    # 2. PPO æ”¶é›†ç»éªŒ
    agent.agent.learn(
        # PPO å†…éƒ¨ä¼šï¼š
        # - æ”¶é›† (obs, action, reward) è½¨è¿¹
        # - è®¡ç®—ä¼˜åŠ¿å‡½æ•°
        # - æ›´æ–°ç­–ç•¥ç½‘ç»œï¼ˆé™åˆ¶æ›´æ–°å¹…åº¦ï¼Œä¿è¯ç¨³å®šï¼‰
    )
```

**PPO å­¦ä¹ æœºåˆ¶**ï¼š
1. **æ”¶é›†ç»éªŒ**ï¼šAgent åœ¨ç¯å¢ƒä¸­æ‰§è¡ŒåŠ¨ä½œï¼Œæ”¶é›† (çŠ¶æ€, åŠ¨ä½œ, å¥–åŠ±) åºåˆ—
2. **è®¡ç®—ä¼˜åŠ¿**ï¼šè¯„ä¼°æ¯ä¸ªåŠ¨ä½œçš„"å¥½å"ï¼ˆç›¸æ¯”å¹³å‡è¡¨ç°ï¼‰
3. **æ›´æ–°ç­–ç•¥**ï¼šè°ƒæ•´ç­–ç•¥ç½‘ç»œï¼Œä½¿å¥½çš„åŠ¨ä½œæ›´å¯èƒ½è¢«é€‰æ‹©
4. **é™åˆ¶æ›´æ–°**ï¼šä½¿ç”¨ clipped objectiveï¼Œé˜²æ­¢ç­–ç•¥å˜åŒ–è¿‡å¤§

---

## 4. å®Œæ•´å·¥ä½œæµç¨‹

### ğŸ“Š ç«¯åˆ°ç«¯æµç¨‹

#### **é˜¶æ®µ 1ï¼šå‡†å¤‡é˜¶æ®µ**

```python
# 1. åŠ è½½è®­ç»ƒå¥½çš„ GNN æ¨¡å‹
gnn_model = load_gnn_model_for_rl()
# GNN å·²ç»åœ¨ Phase 4 è®­ç»ƒå¥½ï¼Œè¿™é‡Œåªç”¨äºç‰¹å¾æå–ï¼ˆå†»ç»“æƒé‡ï¼‰

# 2. åˆ›å»ºç¯å¢ƒå·¥å‚
def make_env():
    return StockTradingEnv(
        start_date=START_DATE,
        end_date=END_DATE,
        gnn_model=gnn_model,  # ç¯å¢ƒå†…éƒ¨ä¼šä½¿ç”¨ GNN ç”ŸæˆçŠ¶æ€
        device=DEVICE
    )

# 3. åˆ›å»º Agent
agent = StockTradingAgent(
    gnn_model=gnn_model,
    env_factory=make_env,
    device=DEVICE,
    learning_rate=1e-5
)
```

#### **é˜¶æ®µ 2ï¼šè®­ç»ƒé˜¶æ®µ**

```python
# è®­ç»ƒ Agent
agent.train(total_timesteps=10000)

# å†…éƒ¨å‘ç”Ÿäº†ä»€ä¹ˆï¼š
# - Agent åœ¨ç¯å¢ƒä¸­æ‰§è¡Œ 10000 æ­¥
# - æ¯ä¸€æ­¥ï¼š
#   1. ç¯å¢ƒæä¾›å½“å‰çŠ¶æ€ï¼ˆä»·æ ¼ã€æŒä»“ã€GNN åµŒå…¥ï¼‰
#   2. Agent é¢„æµ‹åŠ¨ä½œï¼ˆä¹°å…¥/å–å‡º/æŒæœ‰ï¼‰
#   3. ç¯å¢ƒæ‰§è¡ŒåŠ¨ä½œï¼Œè®¡ç®—æ”¶ç›Šå’Œå¥–åŠ±
#   4. PPO æ›´æ–°ç­–ç•¥ï¼Œå­¦ä¹ æ›´å¥½çš„å†³ç­–
# - è®­ç»ƒå®Œæˆåï¼ŒAgent å­¦ä¼šäº†"ä½•æ—¶ä¹°å–"çš„ç­–ç•¥
```

#### **é˜¶æ®µ 3ï¼šè¯„ä¼°é˜¶æ®µ**

```python
# åŠ è½½è®­ç»ƒå¥½çš„ Agent
agent.load(save_path)

# åœ¨æµ‹è¯•é›†ä¸Šå›æµ‹
test_env = make_test_env()
obs, info = test_env.reset()

while not done:
    action, _ = agent.predict(obs, deterministic=True)
    obs, reward, done, truncated, info = test_env.step(action)
    portfolio_values.append(info['portfolio_value'])

# è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
metrics = calculate_financial_metrics(portfolio_values)
# Sharpe Ratio, Cumulative Return, Max Drawdown
```

### ğŸ” å…³é”®ä»£ç ä½ç½®

#### **ç¯å¢ƒå¦‚ä½•ç”ŸæˆçŠ¶æ€** (`rl_environment.py`)

```python
def _get_observation(self):
    # 1. è·å–å½“å‰æŒä»“
    holdings = self.current_holdings / self.portfolio_value  # å½’ä¸€åŒ–
    
    # 2. è·å– GNN åµŒå…¥
    current_graph = self._load_graph_for_date(self.current_date)
    with torch.no_grad():
        embeddings = self.gnn_model.get_embeddings(current_graph)
        embeddings = embeddings.cpu().numpy()
    
    # 3. ç»„åˆçŠ¶æ€
    observation = np.concatenate([holdings, embeddings.flatten()])
    return observation
```

#### **Agent å¦‚ä½•åšå†³ç­–** (`rl_agent.py`)

```python
def predict(self, observation, deterministic=True):
    # PPO agent çš„ç­–ç•¥ç½‘ç»œ
    action, _ = self.agent.predict(observation, deterministic=deterministic)
    # action æ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œå¦‚ [2, 1, 0, 2, ...]
    return action, _
```

#### **ç¯å¢ƒå¦‚ä½•æ‰§è¡ŒåŠ¨ä½œ** (`rl_environment.py`)

```python
def step(self, action):
    # action = [2, 1, 0, 2, ...] æ¯åªè‚¡ç¥¨çš„æ“ä½œ
    
    for stock_idx, action_type in enumerate(action):
        if action_type == 2:  # ä¹°å…¥
            self._buy_stock(stock_idx)
        elif action_type == 0:  # å–å‡º
            self._sell_stock(stock_idx)
        # action_type == 1: æŒæœ‰ï¼Œä¸åšæ“ä½œ
    
    # è®¡ç®—æ”¶ç›Šå’Œå¥–åŠ±
    portfolio_return = (self.portfolio_value - prev_value) / prev_value
    reward = portfolio_return + sharpe_ratio * 0.1 - trading_cost
    
    return next_obs, reward, done, truncated, info
```

---

## 5. æ€»ç»“

### ğŸ¯ Agent çš„æ ¸å¿ƒä»·å€¼

1. **è‡ªåŠ¨åŒ–å†³ç­–**ï¼šä¸éœ€è¦äººå·¥ç¼–å†™äº¤æ˜“è§„åˆ™ï¼Œä»æ•°æ®ä¸­å­¦ä¹ 
2. **è€ƒè™‘å…¨å±€**ï¼šGNN åµŒå…¥è®© Agent ç†è§£è‚¡ç¥¨ä¹‹é—´çš„å…³ç³»
3. **é£é™©æ§åˆ¶**ï¼šå¥–åŠ±å‡½æ•°é¼“åŠ±ç¨³å®šã€ä½é£é™©çš„ç­–ç•¥
4. **æ˜“äºä½¿ç”¨**ï¼šå°è£…å¥½çš„æ¥å£ï¼Œ`train()` å’Œ `predict()` å³å¯

### ğŸ”‘ å…³é”®è®¾è®¡å†³ç­–

1. **ä¸ºä»€ä¹ˆç”¨ GNN + RL**ï¼š
   - GNN æå–è‚¡ç¥¨å…³ç³»ç‰¹å¾ï¼ˆä¼ ç»Ÿ RL åšä¸åˆ°ï¼‰
   - RL å­¦ä¹ åŠ¨æ€äº¤æ˜“ç­–ç•¥ï¼ˆä¼ ç»Ÿç›‘ç£å­¦ä¹ åšä¸åˆ°ï¼‰

2. **ä¸ºä»€ä¹ˆç”¨ PPO**ï¼š
   - ç¨³å®šè®­ç»ƒï¼ˆé‡‘èæ•°æ®å™ªå£°å¤§ï¼‰
   - é€‚åˆç¦»æ•£åŠ¨ä½œç©ºé—´ï¼ˆä¹°å…¥/å–å‡º/æŒæœ‰ï¼‰

3. **ä¸ºä»€ä¹ˆå†»ç»“ GNN**ï¼š
   - GNN å·²ç»åœ¨ Phase 4 è®­ç»ƒå¥½ï¼ˆé¢„æµ‹æ¶¨è·Œï¼‰
   - RL é˜¶æ®µåªå­¦ä¹ "å¦‚ä½•ä½¿ç”¨è¿™äº›ç‰¹å¾åšäº¤æ˜“"

### ğŸ“ˆ å®é™…æ•ˆæœ

ä»ä½ çš„å›æµ‹ç»“æœçœ‹ï¼š
- **Sharpe Ratio: 1.83** âœ… ä¼˜ç§€
- **ç´¯è®¡æ”¶ç›Š: 46.36%** âœ… è‰¯å¥½
- **æœ€å¤§å›æ’¤: 8.23%** âœ… é£é™©æ§åˆ¶è‰¯å¥½

è¿™è¯´æ˜ Agent æˆåŠŸå­¦ä¹ äº†ï¼š
- ä½•æ—¶ä¹°å…¥ï¼ˆæ•æ‰ä¸Šæ¶¨è¶‹åŠ¿ï¼‰
- ä½•æ—¶å–å‡ºï¼ˆé¿å…ä¸‹è·Œï¼‰
- å¦‚ä½•å¹³è¡¡æ”¶ç›Šå’Œé£é™©

---

## 6. è¿›ä¸€æ­¥ç†è§£

### ç±»æ¯”ç†è§£

**Agent å°±åƒä¸€ä¸ªäº¤æ˜“å‘˜**ï¼š
- **çœ¼ç›ï¼ˆGNNï¼‰**ï¼šè§‚å¯Ÿå¸‚åœºï¼Œç†è§£è‚¡ç¥¨å…³ç³»
- **å¤§è„‘ï¼ˆPPOï¼‰**ï¼šå­¦ä¹ äº¤æ˜“ç­–ç•¥ï¼Œåšå†³ç­–
- **æ‰‹ï¼ˆEnvironmentï¼‰**ï¼šæ‰§è¡Œäº¤æ˜“ï¼Œè·å¾—åé¦ˆ

**è®­ç»ƒè¿‡ç¨‹å°±åƒå®ä¹ **ï¼š
- åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­ç»ƒä¹ äº¤æ˜“
- ä»é”™è¯¯ä¸­å­¦ä¹ ï¼ˆäºæŸæ—¶è°ƒæ•´ç­–ç•¥ï¼‰
- ä»æˆåŠŸä¸­å¼ºåŒ–ï¼ˆç›ˆåˆ©æ—¶ä¿æŒç­–ç•¥ï¼‰

### å¸¸è§é—®é¢˜

**Q: ä¸ºä»€ä¹ˆ GNN è¦å†»ç»“ï¼Ÿ**
A: GNN å·²ç»åœ¨ Phase 4 å­¦ä¼šäº†"ç†è§£è‚¡ç¥¨å…³ç³»"ï¼ŒRL é˜¶æ®µåªéœ€è¦å­¦ä¹ "å¦‚ä½•ä½¿ç”¨è¿™äº›ç†è§£åšäº¤æ˜“"ã€‚å¦‚æœåŒæ—¶è®­ç»ƒï¼Œå¯èƒ½ä¼šç ´å GNN å·²ç»å­¦åˆ°çš„çŸ¥è¯†ã€‚

**Q: PPO å¦‚ä½•å­¦ä¹ ï¼Ÿ**
A: é€šè¿‡è¯•é”™ã€‚Agent å°è¯•ä¸åŒçš„äº¤æ˜“ç­–ç•¥ï¼Œå¦‚æœç­–ç•¥å¸¦æ¥é«˜æ”¶ç›Šï¼ˆé«˜å¥–åŠ±ï¼‰ï¼ŒPPO ä¼šå¢åŠ è¿™ä¸ªç­–ç•¥çš„æ¦‚ç‡ï¼›å¦‚æœå¸¦æ¥äºæŸï¼ˆä½å¥–åŠ±ï¼‰ï¼Œä¼šå‡å°‘æ¦‚ç‡ã€‚

**Q: çŠ¶æ€ä¸ºä»€ä¹ˆåŒ…å«æŒä»“ï¼Ÿ**
A: Agent éœ€è¦çŸ¥é“"æˆ‘ç°åœ¨æŒæœ‰ä»€ä¹ˆ"ï¼Œæ‰èƒ½å†³å®š"åº”è¯¥ä¹°å…¥ä»€ä¹ˆ"æˆ–"åº”è¯¥å–å‡ºä»€ä¹ˆ"ã€‚æ²¡æœ‰æŒä»“ä¿¡æ¯ï¼ŒAgent æ— æ³•åšå‡ºåˆç†çš„å†³ç­–ã€‚

---

å¸Œæœ›è¿™ä¸ªè§£é‡Šå¸®åŠ©ä½ ç†è§£ Agent çš„å·¥ä½œåŸç†ï¼ğŸš€

