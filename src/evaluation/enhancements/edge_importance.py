# enhancement_edge_importance.py
"""
Edge Importance Analysis for Deep Analysis Enhancement
Identifies which edges matter most, analyzes sector subgraphs, and correlation vs fundamental importance.
"""

import torch
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple, Any
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
import sys
import networkx as nx

PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from src.training.transformer_trainer import RoleAwareGraphTransformer
from src.utils.graph_loader import load_graph_data
from src.utils.data import load_data_file

from src.utils.paths import MODELS_DIR, RESULTS_DIR, MODELS_PLOTS_DIR as PLOTS_DIR
RESULTS_DIR.mkdir(parents=True, exist_ok=True)
PLOTS_DIR.mkdir(parents=True, exist_ok=True)

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


def extract_attention_weights_per_edge_type(
    model: RoleAwareGraphTransformer,
    data,
    edge_types: List[Tuple]
) -> Dict[Tuple, torch.Tensor]:
    """
    Extract attention weights for each edge type.
    
    Returns:
        Dictionary mapping edge_type -> attention weights
    """
    model.eval()
    attention_weights = {}
    
    with torch.no_grad():
        x_dict = data.x_dict
        edge_index_dict = data.edge_index_dict
        
        # Get PEARL embeddings
        x = x_dict['stock']
        pearl_pe = model.pearl_embedding(x, edge_index_dict)
        x_with_pe = torch.cat([x, pearl_pe], dim=1)
        x_dict['stock'] = x_with_pe
        
        # Extract attention from first layer
        if len(model.convs) > 0:
            conv = model.convs[0]
            
            # Get attention weights for each edge type
            for edge_type in edge_types:
                if edge_type in edge_index_dict:
                    # Access attention weights from GATv2Conv
                    # Note: This requires accessing internal attention weights
                    # Simplified version - would need model modification for full access
                    edge_index = edge_index_dict[edge_type]
                    num_edges = edge_index.size(1)
                    
                    # Placeholder: In full implementation, would extract actual attention weights
                    # For now, use edge count as proxy
                    attention_weights[edge_type] = torch.ones(num_edges) / num_edges
    
    return attention_weights


def analyze_edge_importance(
    model: RoleAwareGraphTransformer,
    test_dates: List[pd.Timestamp],
    tickers: List[str],
    edge_types: List[Tuple]
) -> Dict[str, Any]:
    """
    Identify which edges matter most for predictions.
    
    Returns:
        Dictionary with edge importance analysis
    """
    print("\n" + "="*60)
    print("üîç Analyzing Edge Importance")
    print("="*60)
    
    edge_importance_stats = defaultdict(lambda: {'total_attention': 0, 'count': 0, 'avg_attention': 0})
    
    model.eval()
    with torch.no_grad():
        for date in test_dates[:50]:  # Sample 50 dates
            try:
                data = load_graph_data(date, tickers)
                if data is None:
                    continue
                
                data = data.to(DEVICE)
                
                # Extract attention weights
                attention_weights = extract_attention_weights_per_edge_type(model, data, edge_types)
                
                # Aggregate statistics
                for edge_type, weights in attention_weights.items():
                    edge_importance_stats[edge_type]['total_attention'] += weights.sum().item()
                    edge_importance_stats[edge_type]['count'] += 1
                    edge_importance_stats[edge_type]['avg_attention'] = (
                        edge_importance_stats[edge_type]['total_attention'] / 
                        max(edge_importance_stats[edge_type]['count'], 1)
                    )
            
            except Exception as e:
                continue
    
    # Calculate importance rankings
    edge_importance_ranked = sorted(
        edge_importance_stats.items(),
        key=lambda x: x[1]['avg_attention'],
        reverse=True
    )
    
    print(f"\nüìä Edge Importance Rankings:")
    for i, (edge_type, stats) in enumerate(edge_importance_ranked):
        edge_name = '_'.join(edge_type) if isinstance(edge_type, tuple) else str(edge_type)
        print(f"   {i+1}. {edge_name}: Avg Attention = {stats['avg_attention']:.4f}")
    
    return {
        'edge_importance_stats': dict(edge_importance_stats),
        'rankings': [(str(et), stats) for et, stats in edge_importance_ranked]
    }


def analyze_sector_subgraphs(
    model: RoleAwareGraphTransformer,
    test_dates: List[pd.Timestamp],
    tickers: List[str]
) -> Dict[str, Any]:
    """
    Analyze sector-specific subgraphs and their importance.
    
    Returns:
        Dictionary with sector subgraph analysis
    """
    print("\n" + "="*60)
    print("üîç Analyzing Sector Subgraphs")
    print("="*60)
    
    # Load sector mapping
    sector_file = PROJECT_ROOT / "data" / "raw" / "static_sector_industry.csv"
    ticker_to_sector = {}
    sector_to_tickers = defaultdict(list)
    
    if sector_file.exists():
        sector_df = pd.read_csv(sector_file)
        for _, row in sector_df.iterrows():
            ticker = row['Ticker']
            sector = row.get('Sector', 'Unknown')
            ticker_to_sector[ticker] = sector
            sector_to_tickers[sector].append(ticker)
    
    sector_performance = {}
    
    for sector, sector_tickers in sector_to_tickers.items():
        if len(sector_tickers) < 3:  # Skip sectors with too few stocks
            continue
        
        print(f"\n   Analyzing {sector} sector ({len(sector_tickers)} stocks)...")
        
        # Filter to sector stocks
        sector_indices = [i for i, t in enumerate(tickers) if t in sector_tickers]
        
        if len(sector_indices) < 2:
            continue
        
        # Analyze predictions for this sector
        sector_predictions = []
        sector_targets = []
        
        model.eval()
        with torch.no_grad():
            for date in test_dates[:30]:  # Sample
                try:
                    data = load_graph_data(date, tickers)
                    if data is None:
                        continue
                    
                    data = data.to(DEVICE)
                    targets = load_targets().get(date)
                    if targets is None:
                        continue
                    
                    # Get predictions
                    logits = model(data)
                    probs = torch.softmax(logits, dim=1)
                    preds = torch.argmax(probs, dim=1)
                    
                    # Extract sector-specific predictions
                    for idx in sector_indices:
                        if idx < len(preds) and idx < len(targets):
                            sector_predictions.append(preds[idx].item())
                            sector_targets.append(targets[idx].item())
                
                except Exception as e:
                    continue
        
        if sector_predictions:
            accuracy = np.mean(np.array(sector_predictions) == np.array(sector_targets))
            sector_performance[sector] = {
                'accuracy': accuracy,
                'n_predictions': len(sector_predictions),
                'n_stocks': len(sector_indices)
            }
            
            print(f"     Accuracy: {accuracy*100:.2f}%")
    
    return {
        'sector_performance': sector_performance,
        'sector_to_tickers': dict(sector_to_tickers)
    }


def analyze_correlation_vs_fundamental_importance(
    model: RoleAwareGraphTransformer,
    test_dates: List[pd.Timestamp],
    tickers: List[str]
) -> Dict[str, Any]:
    """
    Compare importance of correlation edges vs fundamental similarity edges.
    
    Returns:
        Dictionary with comparison analysis
    """
    print("\n" + "="*60)
    print("üîç Analyzing Correlation vs Fundamental Importance")
    print("="*60)
    
    correlation_importance = []
    fundamental_importance = []
    
    model.eval()
    with torch.no_grad():
        for date in test_dates[:50]:  # Sample
            try:
                data = load_graph_data(date, tickers)
                if data is None:
                    continue
                
                data = data.to(DEVICE)
                
                # Count edges of each type
                edge_index_dict = data.edge_index_dict
                
                corr_edges = 0
                fund_edges = 0
                
                for edge_type in edge_index_dict.keys():
                    edge_name = str(edge_type)
                    num_edges = edge_index_dict[edge_type].size(1)
                    
                    if 'correlation' in edge_name.lower():
                        corr_edges += num_edges
                    elif 'fund' in edge_name.lower() or 'similarity' in edge_name.lower():
                        fund_edges += num_edges
                
                correlation_importance.append(corr_edges)
                fundamental_importance.append(fund_edges)
            
            except Exception as e:
                continue
    
    if correlation_importance and fundamental_importance:
        avg_corr = np.mean(correlation_importance)
        avg_fund = np.mean(fundamental_importance)
        
        print(f"\nüìä Edge Type Comparison:")
        print(f"   Average Correlation Edges: {avg_corr:.0f}")
        print(f"   Average Fundamental Edges: {avg_fund:.0f}")
        print(f"   Ratio (Corr/Fund): {avg_corr/max(avg_fund, 1):.2f}")
        
        return {
            'correlation_importance': {
                'mean': avg_corr,
                'std': np.std(correlation_importance),
                'values': correlation_importance
            },
            'fundamental_importance': {
                'mean': avg_fund,
                'std': np.std(fundamental_importance),
                'values': fundamental_importance
            },
            'ratio': avg_corr / max(avg_fund, 1)
        }
    
    return {}


def visualize_edge_importance(
    edge_importance: Dict,
    sector_subgraphs: Dict,
    corr_vs_fund: Dict,
    output_dir: Path
):
    """Create visualizations for edge importance analysis."""
    print("\nüìä Creating Edge Importance Visualizations...")
    
    # 1. Edge Importance Rankings
    if edge_importance.get('rankings'):
        fig, ax = plt.subplots(figsize=(12, 6))
        
        edge_names = [r[0] for r in edge_importance['rankings']]
        importances = [r[1]['avg_attention'] for r in edge_importance['rankings']]
        
        ax.barh(edge_names, importances)
        ax.set_xlabel('Average Attention Weight')
        ax.set_title('Edge Type Importance Rankings')
        ax.grid(True, alpha=0.3, axis='x')
        
        plt.tight_layout()
        plt.savefig(output_dir / 'edge_importance_rankings.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("   ‚úÖ Saved: edge_importance_rankings.png")
    
    # 2. Sector Performance Comparison
    if sector_subgraphs.get('sector_performance'):
        fig, ax = plt.subplots(figsize=(12, 6))
        
        sectors = list(sector_subgraphs['sector_performance'].keys())
        accuracies = [sector_subgraphs['sector_performance'][s]['accuracy']*100 for s in sectors]
        
        ax.bar(sectors, accuracies)
        ax.set_ylabel('Accuracy (%)')
        ax.set_title('GNN Performance by Sector')
        ax.tick_params(axis='x', rotation=45)
        ax.grid(True, alpha=0.3, axis='y')
        
        plt.tight_layout()
        plt.savefig(output_dir / 'sector_subgraph_performance.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("   ‚úÖ Saved: sector_subgraph_performance.png")
    
    # 3. Correlation vs Fundamental Comparison
    if corr_vs_fund.get('correlation_importance'):
        fig, ax = plt.subplots(figsize=(10, 6))
        
        dates_range = range(len(corr_vs_fund['correlation_importance']['values']))
        ax.plot(dates_range, corr_vs_fund['correlation_importance']['values'], 
               label='Correlation Edges', alpha=0.7)
        ax.plot(dates_range, corr_vs_fund['fundamental_importance']['values'], 
               label='Fundamental Edges', alpha=0.7)
        ax.set_xlabel('Time Step')
        ax.set_ylabel('Number of Edges')
        ax.set_title('Correlation vs Fundamental Edge Count Over Time')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(output_dir / 'correlation_vs_fundamental_edges.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("   ‚úÖ Saved: correlation_vs_fundamental_edges.png")


def main():
    """Main edge importance analysis pipeline."""
    print("üöÄ Edge Importance Analysis")
    print("="*60)
    
    # Load model
    print("\nüìÅ Loading GNN model...")
    model_path = MODELS_DIR / 'core_transformer_model.pt'
    if not model_path.exists():
        print(f"‚ùå Model not found: {model_path}")
        return
    
    # Load sample graph to get dimensions
    from src.utils.graph_loader import load_graph_data
    sample_graph = list((PROJECT_ROOT / "data" / "graphs").glob('graph_t_*.pt'))[0]
    temp_data = torch.load(sample_graph, weights_only=False)
    INPUT_DIM = temp_data['stock'].x.shape[1]
    
    # Use correct model parameters from config
    from src.training.transformer_trainer import HIDDEN_CHANNELS, OUT_CHANNELS, NUM_LAYERS, NUM_HEADS
    model = RoleAwareGraphTransformer(INPUT_DIM, HIDDEN_CHANNELS, OUT_CHANNELS, NUM_LAYERS, NUM_HEADS).to(DEVICE)
    model.load_state_dict(torch.load(model_path, map_location=DEVICE, weights_only=False), strict=False)
    model.eval()
    print("‚úÖ Model loaded")
    
    # Get test dates and tickers (simplified)
    # Get sample dates from graph files
    graph_files = list((PROJECT_ROOT / "data" / "graphs").glob('graph_t_*.pt'))
    test_dates = [pd.to_datetime(f.stem.split('_')[-1], format='%Y%m%d') for f in graph_files[:50]]
    
    # Get tickers from first graph
    if graph_files:
        sample_data = torch.load(graph_files[0], weights_only=False)
        if 'tickers' in sample_data:
            tickers = sample_data['tickers']
        else:
            tickers = [f"Stock_{i}" for i in range(50)]
    else:
        tickers = [f"Stock_{i}" for i in range(50)]
    
    # Define edge types
    edge_types = [
        ('stock', 'sector_industry', 'stock'),
        ('stock', 'competitor', 'stock'),
        ('stock', 'supply_chain', 'stock'),
        ('stock', 'rolling_correlation', 'stock'),
        ('stock', 'fund_similarity', 'stock')
    ]
    
    # Run analyses
    print("\n" + "="*60)
    print("Running Edge Importance Analyses...")
    print("="*60)
    
    # 1. Edge Importance Analysis
    edge_importance = analyze_edge_importance(model, test_dates, tickers, edge_types)
    
    # 2. Sector Subgraph Analysis
    sector_subgraphs = analyze_sector_subgraphs(model, test_dates, tickers)
    
    # 3. Correlation vs Fundamental Analysis
    corr_vs_fund = analyze_correlation_vs_fundamental_importance(model, test_dates, tickers)
    
    # Visualize
    visualize_edge_importance(edge_importance, sector_subgraphs, corr_vs_fund, PLOTS_DIR)
    
    # Save results
    import json
    results = {
        'edge_importance': edge_importance,
        'sector_subgraphs': sector_subgraphs,
        'correlation_vs_fundamental': corr_vs_fund
    }
    
    results_file = RESULTS_DIR / 'edge_importance_analysis.json'
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    print(f"\n‚úÖ Results saved to: {results_file}")
    print("\nüéâ Edge Importance Analysis Complete!")


if __name__ == "__main__":
    main()

