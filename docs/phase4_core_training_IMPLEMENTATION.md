# Phase 4: Core Transformer Training - Implementation Guide

## Overview

**File**: `scripts/phase4_core_training.py`  
**Purpose**: Train advanced role-aware graph transformer with PEARL embeddings  
**Dependencies**: torch, torch-geometric, sklearn, custom components  
**Input**: Graph snapshots from Phase 2  
**Output**: State-of-the-art GNN model for stock prediction

---

## What Does This File Do?

This script implements the **core research contribution**: A sophisticated GNN that combines:

1. **PEARL Positional Embeddings** - Learnable graph structure encoding
2. **Relation-Aware Attention** - Different processing for each edge type
3. **HeteroConv Architecture** - Explicitly handles multiple edge types
4. **Mini-Batch Training** - Neighbor sampling for scalability (optional)

**This is the main research model!** üß†

---

## Why This vs Baseline (Phase 3)?

### Feature Comparison

| Feature | Phase 3 Baseline | Phase 4 Core |
|---------|-----------------|--------------|
| **Architecture** | GAT (standard) | Role-Aware Transformer |
| **Edge Handling** | Concatenate all edges | Separate processing per type |
| **Positional Encoding** | None | PEARL (learnable) |
| **Attention** | Standard GAT | Relation-aware attention |
| **Parameters** | ~100K | ~1M+ |
| **Training Time** | 15-20 min | 60-90 min |
| **Expected F1** | 0.60-0.65 | 0.65-0.72 |

**Why Both**:
- Baseline: Simple, fast, comparison point
- Core: State-of-the-art, research contribution

---

## Model Architecture

### RoleAwareGraphTransformer

```python
class RoleAwareGraphTransformer(torch.nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim, num_layers, num_heads):
```

#### Three-Stage Architecture

**Stage 1: PEARL Positional Embedding**
```python
self.pearl_embedding = PEARLPositionalEmbedding(in_dim, PE_DIM=32)
pearl_pe = self.pearl_embedding(x, edge_index_dict)
x_with_pe = torch.cat([x, pearl_pe], dim=1)
```

**Purpose**: Encode graph structure information
- Node features: `[N, F]` (what the stock is)
- Positional encoding: `[N, 32]` (where the stock is in graph)
- Combined: `[N, F+32]`

**Why Needed**:
- GNNs are permutation invariant (node order doesn't matter)
- But stock's **position in graph** matters:
  - Central stocks (many connections): Market leaders
  - Peripheral stocks: Niche players
- PEARL learns to encode this structural information

**Stage 2: Relation-Aware GNN Layers**
```python
for i in range(num_layers):
    conv = HeteroConv({
        edge_type: RelationAwareGATv2Conv(...)
        for edge_type in metadata
    }, aggr='sum')
```

**HeteroConv**: Processes each edge type separately
```
Correlation edges: W_corr √ó h_neighbors
Sector edges: W_sector √ó h_neighbors
Competitor edges: W_comp √ó h_neighbors

Final: h_new = aggregate([corr_out, sector_out, comp_out])
```

**Why Separate Weights**:
- Different relationships need different processing
- Sector edge: Strong similarity signal
- Competitor edge: May be inverse signal
- Model learns appropriate weights

**Stage 3: Relation-Aware Aggregation**
```python
aggregator = RelationAwareAggregator(hidden_dim, metadata)
x = aggregator(relation_outputs)
```

**Purpose**: Combine outputs from different edge types intelligently

**Mathematical Formula**:
```
h_final = Œ£_r w_r √ó Transform_r(h_r)

where:
r ‚àà edge types
w_r = learned weight for relation r (via softmax)
Transform_r = MLP specific to relation r
```

**Why Not Simple Sum**:
- Different edge types have different importance
- Weights learned from data
- More expressive than fixed aggregation

---

## PEARL Integration

### What is PEARL?

**PEARL**: **P**osition-**E**ncoding-**A**ware **R**epresentation **L**earning

**From Paper** (T√∂nshoff et al., 2022):
```
PE(i) = Learn(G, i)

where:
G = graph structure
i = node index
PE(i) = vector encoding node i's structural role
```

**Example**:
```
Node 0 (Apple): Central, many connections
PE_0 = [0.8, 0.2, ...]  (high centrality encoding)

Node 49 (Small Stock): Peripheral, few connections
PE_49 = [0.1, 0.9, ...]  (low centrality encoding)
```

### Why PEARL Matters

**Problem**: Standard GNNs only aggregate neighbor features
```
h_i = AGG({h_j : j ‚àà N(i)})
```

No direct encoding of:
- Node degree
- Centrality
- Clustering coefficient
- Structural role

**PEARL Solution**: Explicitly learn these
```
h_i = AGG({h_j : j ‚àà N(i)}) ‚äï PEARL(G, i)
                                    ‚Üë
                          Structural information
```

**For Stock Graphs**:
- Central stocks (Apple, Microsoft): Market movers
- Their structural position is **informative**
- PEARL captures this automatically

---

## RelationAwareGATv2Conv

### Enhanced Attention

```python
class RelationAwareGATv2Conv(GATv2Conv):
    def __init__(self, ..., edge_type=None):
        # Add edge-type specific parameters
        self.edge_embedding = nn.Parameter(...)
        self.edge_attention_bias = nn.Parameter(...)
```

#### Standard GATv2

**Formula**:
```
Œ±_{ij} = softmax_j(LeakyReLU(a^T [W h_i || W h_j]))
```

**Attention is same** for all edge types

#### Relation-Aware Enhancement

**Formula**:
```
Œ±_{ij}^{(r)} = softmax_j(LeakyReLU(a_r^T [W_r h_i || W_r h_j] + b_r))

where:
r = edge type
a_r = attention vector for relation r
W_r = transformation matrix for relation r
b_r = bias for relation r
```

**Attention is different** for each edge type!

**Example**:
```
For AAPL ‚Üê MSFT connection:

If edge type = 'sector':
Œ±_{AAPL,MSFT}^{sector} = 0.7  (high - same sector)

If edge type = 'competitor':
Œ±_{AAPL,MSFT}^{comp} = 0.3  (low - not direct competitors)

If edge type = 'correlation':
Œ±_{AAPL,MSFT}^{corr} = 0.8  (high - price movements correlated)
```

**Benefit**: Model learns edge-type-specific importance

---

## Mini-Batch Training (Optional)

### Why Mini-Batch?

**Full-Batch Training** (Current Default):
```python
for date in train_dates:
    # Load entire graph (50 nodes, ~300 edges)
    graph = load_graph(date)
    loss = train(model, graph)  # Process all 50 nodes
```

**Problem for Large Graphs**:
- 500 stocks ‚Üí GPU memory overflow
- 5000 stocks ‚Üí Impossible to fit in memory

**Mini-Batch Solution**:
```python
for date in train_dates:
    graph = load_graph(date)
    loader = NeighborLoader(graph, batch_size=128)
    for batch in loader:
        # Process only 128 target nodes + their neighbors
        loss = train(model, batch)
```

### Neighbor Sampling

**NeighborLoader Parameters**:
```python
loader = NeighborLoader(
    data,
    num_neighbors=[15, 10],  # Sample 15 neighbors in layer 1, 10 in layer 2
    batch_size=128
)
```

**How It Works**:

**Example**: Training node 0 (AAPL) with 2 layers

**Layer 1**: Sample 15 neighbors
```
AAPL ‚Üê [MSFT, GOOGL, AMZN, ..., NVDA]  (15 stocks)
```

**Layer 2**: For each of those 15, sample 10 neighbors
```
MSFT ‚Üê [10 stocks]
GOOGL ‚Üê [10 stocks]
...
Total: 15 √ó 10 = 150 stocks
```

**Computational Graph**:
```
Layer 2 Neighbors (150 stocks)
    ‚Üì Aggregate
Layer 1 Neighbors (15 stocks)
    ‚Üì Aggregate
Target Node (AAPL)
```

**Memory**: Only need 150 stocks, not all 500!

### When to Enable

```python
ENABLE_MINI_BATCH = False  # Default: disabled for 50 stocks
```

**Enable if**:
- More than 200 stocks
- GPU memory limitations
- Want faster training per epoch

**Disable if**:
- Small graph (<100 nodes)
- Sufficient GPU memory
- Want best accuracy (full neighborhood information)

---

## Training Loop Differences from Phase 3

### Simplified (Fewer Features)

**Phase 3 Has**:
- ‚úÖ Class imbalance handling
- ‚úÖ Checkpointing
- ‚úÖ Early stopping
- ‚úÖ LR scheduler
- ‚úÖ TensorBoard
- ‚úÖ ROC-AUC
- ‚úÖ Confusion matrix

**Phase 4 Has**:
- ‚ùå Class imbalance (uses standard CE)
- ‚ùå Checkpointing
- ‚ùå Early stopping
- ‚ùå LR scheduler
- ‚ùå TensorBoard
- ‚úÖ Basic metrics (Acc, F1)

**Why Simpler**:
- Phase 3 is **production training script**
- Phase 4 is **research prototype**
- Focus on model architecture, not training infrastructure

**Future Work**: Merge Phase 3's training features into Phase 4

---

## Configuration Highlights

### Hyperparameters

```python
HIDDEN_CHANNELS = 256     # Larger than baseline (64)
NUM_LAYERS = 2            # Same as baseline
NUM_HEADS = 4             # Same as baseline
LEARNING_RATE = 0.0005    # Lower than baseline (0.001)
NUM_EPOCHS = 1           # Reduced for testing
```

#### Why Lower Learning Rate?

**More Parameters**: Baseline ~100K, Core ~1M

**Optimization Landscape**:
```
Simple Model (Baseline):
Loss landscape is smooth
‚Üí Can use larger LR (0.001)

Complex Model (Core):
Loss landscape has sharper features
‚Üí Need smaller LR (0.0005) to avoid divergence
```

**Mathematical Intuition**:
```
Parameter update: Œ∏ ‚Üê Œ∏ - LR √ó ‚àáL

More parameters ‚Üí Higher-dimensional space
‚Üí Easier to overshoot with large LR
‚Üí Use smaller LR for stability
```

---

## Edge Types Processing

### Metadata Definition

```python
metadata = [
    ('stock', 'sector_industry', 'stock'),
    ('stock', 'competitor', 'stock'),
    ('stock', 'supply_chain', 'stock'),
    ('stock', 'rolling_correlation', 'stock'),
    ('stock', 'fund_similarity', 'stock')
]
```

**Schema**: `(source_type, edge_type, target_type)`

**For Stock Graph**: All nodes are 'stock' type
- Homogeneous nodes, heterogeneous edges
- Simpler than fully heterogeneous (multiple node types)

### HeteroConv Mechanics

**Creating Separate Convolutions**:
```python
conv = HeteroConv({
    edge_type: RelationAwareGATv2Conv(in_dim, out_dim, ...)
    for edge_type in metadata
}, aggr='sum')
```

**Expands to**:
```python
conv = HeteroConv({
    ('stock', 'sector_industry', 'stock'): GATConv_1,
    ('stock', 'competitor', 'stock'): GATConv_2,
    ('stock', 'supply_chain', 'stock'): GATConv_3,
    ('stock', 'rolling_correlation', 'stock'): GATConv_4,
    ('stock', 'fund_similarity', 'stock'): GATConv_5
}, aggr='sum')
```

**Each GATConv** has its own parameters!
- 5 edge types √ó ~200K params = 1M+ total parameters

**Forward Pass**:
```python
# For each edge type:
out_sector = GATConv_1(x, edge_index_sector)
out_comp = GATConv_2(x, edge_index_comp)
out_supply = GATConv_3(x, edge_index_supply)
out_corr = GATConv_4(x, edge_index_corr)
out_fund = GATConv_5(x, edge_index_fund)

# Aggregate (sum)
out = out_sector + out_comp + out_supply + out_corr + out_fund
```

---

## Why This Model is Advanced

### Research Contributions

**1. PEARL Integration**
- First application of PEARL to financial graphs
- Encodes market structure (central vs peripheral stocks)
- Improves over fixed positional encodings (Laplacian eigenvectors)

**2. Relation-Aware Processing**
- Different edge types processed differently
- Learns which relationships matter for stock prediction
- Enables ablation studies (remove edge types)

**3. Heterogeneous Architecture**
- Standard GNNs: Single message passing function
- This model: Edge-type-specific message passing
- More expressive, better performance

### Theoretical Foundation

**Message Passing Neural Networks (MPNN) Framework**:
```
m_{ij}^{(l)} = Message^{(l)}(h_i^{(l)}, h_j^{(l)}, e_{ij})
m_i^{(l)} = Aggregate({m_{ij}^{(l)} : j ‚àà N(i)})
h_i^{(l+1)} = Update^{(l)}(h_i^{(l)}, m_i^{(l)})
```

**This Model's Enhancement**:
```
m_{ij}^{(l,r)} = Message_r^{(l)}(h_i^{(l)}, h_j^{(l)}, e_{ij}, r)
                              ‚Üë
                    Edge-type specific function

m_i^{(l)} = Aggregate({w_r √ó m_i^{(l,r)} : r ‚àà R})
                        ‚Üë
              Learned relation weights
```

**Innovation**: Both message function AND aggregation are relation-aware

---

## Mini-Batch Training Details

### NeighborLoader

```python
loader = NeighborLoader(
    data,
    num_neighbors=[15, 10],
    batch_size=128,
    input_nodes=('stock', target_nodes)
)
```

#### How It Samples

**Batch Construction**:
```
1. Select 128 target nodes (e.g., stocks 0-127)
2. For each target node:
   - Sample 15 neighbors (layer 1 receptive field)
   - For each of those 15:
     - Sample 10 neighbors (layer 2 receptive field)
3. Create subgraph with all sampled nodes
4. Return mini-batch
```

**Receptive Field**:
```
2-layer GNN with [15, 10] sampling:
- Target node sees up to 15 √ó 10 = 150 nodes
- Not all 500 nodes in full graph
- Approximation of full-batch
```

#### Trade-offs

**Full-Batch** (all nodes):
- ‚úÖ Exact gradients
- ‚úÖ Better accuracy
- ‚ùå High memory
- ‚ùå Slow for large graphs

**Mini-Batch** (sampled):
- ‚úÖ Low memory
- ‚úÖ Fast for large graphs
- ‚ùå Approximate gradients
- ‚ùå Slightly lower accuracy

**For 50 Stocks**: Full-batch is fine  
**For 500+ Stocks**: Mini-batch necessary

---

## AMP (Automatic Mixed Precision)

### What is AMP?

**Standard Training** (FP32):
```python
# All tensors in 32-bit floating point
x = torch.randn(100, 64, dtype=torch.float32)
```

**Mixed Precision** (FP16 + FP32):
```python
with torch.cuda.amp.autocast():
    # Forward pass in FP16 (faster)
    out = model(x)

# Backward pass scales gradients to prevent underflow
scaler.scale(loss).backward()
scaler.step(optimizer)
```

### Why Mixed Precision?

**FP16 (Half Precision)**:
- 16 bits vs 32 bits
- 2√ó faster on modern GPUs
- 2√ó less memory

**Range**: ¬±65,504 (smaller than FP32's ¬±3.4√ó10¬≥‚Å∏)

**Problem**: Gradient underflow
```
FP32 gradient: 0.000001 (representable)
FP16 gradient: 0 (underflow to zero)
```

**Solution**: Gradient scaling
```python
scaler = torch.cuda.amp.GradScaler()
loss_scaled = loss √ó 1000  # Scale up
loss_scaled.backward()     # Gradients are large enough
scaler.step(optimizer)     # Scale back down
```

### When Enabled

```python
ENABLE_AMP = torch.cuda.is_available()
```

**Only on GPU**: CPU doesn't benefit from FP16

**Speedup**: 
- GPU: 1.5-2√ó faster training
- CPU: No benefit (may be slower)

---

## Output and Integration

### Saved Model

```python
torch.save(model.state_dict(), MODELS_DIR / 'core_transformer_model.pt')
```

**Size**: ~5-20 MB (depending on parameters)

**Usage in Phase 5 (RL)**:
```python
# Load model
model = RoleAwareGraphTransformer(...)
model.load_state_dict(torch.load('core_transformer_model.pt'))

# Freeze for RL (use as feature extractor)
for param in model.parameters():
    param.requires_grad = False

# Generate embeddings
embeddings = model(graph)  # Use in RL observation
```

---

## Comparison with Phase 3

### Expected Performance

**Baseline (Phase 3)**:
```
Test F1: 0.62-0.65
Test Accuracy: 0.60-0.63
```

**Core (Phase 4)**:
```
Test F1: 0.66-0.72  (5-10% improvement)
Test Accuracy: 0.64-0.68
```

**Why Improvement**:
- PEARL captures graph structure
- Relation-aware processing more expressive
- More parameters for complex patterns

**Is 5-10% Worth It?**

**For Research**: Yes!
- Demonstrates architectural improvements
- Publishable contribution
- Ablation studies prove each component's value

**For Production**: Depends
- 5% F1 improvement in trading can be huge
- But training time 4√ó longer
- Complexity vs performance trade-off

---

## Limitations

### Current Issues

1. **No Advanced Training Features**
   - Missing checkpointing, early stopping, etc.
   - Should merge with Phase 3's infrastructure

2. **Single Epoch Default**
   ```python
   NUM_EPOCHS = 1  # For quick testing
   ```
   - Need to increase for real training
   - Should be 20-30 epochs

3. **No Class Imbalance Handling**
   - Uses standard cross-entropy
   - Should add weighted/focal loss

### Future Improvements

1. **Merge with Phase 3 Features**:
   ```python
   # Add to Phase 4:
   - Class imbalance handling
   - Checkpointing system
   - Early stopping
   - TensorBoard logging
   ```

2. **Gradient Clipping**:
   ```python
   torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
   ```
   - Prevents exploding gradients in deep models

3. **Warmup Learning Rate**:
   ```python
   for epoch in range(warmup_epochs):
       lr = min_lr + (max_lr - min_lr) * (epoch / warmup_epochs)
   ```
   - Helps large models converge

---

## Summary

**Purpose**: Train state-of-the-art GNN for stock prediction  
**Key Innovations**: PEARL positional encoding, relation-aware attention  
**Architecture**: Multi-layer heterogeneous graph transformer  
**Expected Improvement**: 5-10% over baseline

**This is the research contribution of the project!** üèÜ

---

**Last Updated**: 2025-11-02  
**Code Style**: Research-focused with detailed architecture explanation [[memory:3128464]]  
**Language**: English comments [[memory:3128459]]

